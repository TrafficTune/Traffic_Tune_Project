{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a145c81c2dbf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Traffic Tune - Optimizing Traffic Signals with Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the Traffic Tune POC notebook. Our project focused on optimizing traffic signal control using reinforcement learning. Traffic congestion is a major problem in urban areas, leading to increased travel times, fuel consumption, and pollution. Traditional traffic signal control systems often struggle to adapt to dynamic traffic conditions, resulting in suboptimal traffic flow.\n",
    "\n",
    "Traffic Tune is a recommendation system that leverages reinforcement learning to dynamically adjust traffic signals at intersections. By learning from traffic patterns in real-time, Traffic Tune aims to improve traffic flow, reduce congestion, and enhance overall transportation efficiency.\n",
    "\n",
    "In this POC, we will demonstrate how to train a reinforcement learning agent to optimize traffic signal control in a simulated environment. We will use the SUMO (Simulation of Urban MObility) traffic simulation tool and the Stable Baselines3 library to train a Deep Q-Network (DQN) agent to learn an optimal traffic signal control policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476f7866dc519ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup and Installations"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import env_manager as env_manager\n",
    "from Old_Files import ppo_trainer as ppo_trainer"
   ],
   "id": "325ce834970de795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_intersection_to_train = 4 # Choose which intersection you want to train\n",
    "\n",
    "experiment_type = \"SingleAgent\" # Choose the experiment_type: SingleAgent | MultiAgent"
   ],
   "id": "c1aea9551bb990a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "env setup",
   "id": "9ca6bdb038bc5642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "manager = env_manager.EnvManager(f\"{experiment_type}Environment\", \"env_config.json\", json_id=f\"intersection_{num_intersection_to_train}\")\n",
    "generator = manager.env_generator(f\"Nets/intersection_{num_intersection_to_train}/route_xml_path_intersection_{num_intersection_to_train}.txt\")\n",
    "rou , csv = next(generator)\n",
    "env_kwargs = manager.initialize_env(rou, csv)\n",
    "\n",
    "print(f\"\\nEnv creat for intersection_{num_intersection_to_train}\",\n",
    "      \"\\nNet path:\", manager.kwargs[\"net_file\"],\n",
    "      \"\\nRoute path:\", rou,\n",
    "      \"\\nCsv path:\", csv)"
   ],
   "id": "3dbaca67cce7f5c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "agent setup",
   "id": "b64cd532db18accd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ppo_agent = ppo_trainer.PPOTrainer(\"ppo_config.json\", manager, experiment_type=experiment_type)\n",
    "ppo_agent.build_config()"
   ],
   "id": "8309087ffdba4187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "agent training",
   "id": "ffd34e669e6861c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results = ppo_agent.train()",
   "id": "94b0bb667ed4bf91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(results.get_best_result())",
   "id": "30f290fd8430f049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "agent prediction",
   "id": "51f4ac4280880d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ppo_agent.evaluate(results=results, kwargs=env_kwargs)\n",
    "best = results.get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "print(best)"
   ],
   "id": "106ea8376d47f062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rou , csv = next(generator)\n",
    "print(rou)\n",
    "print(csv)\n",
    "env_kwargs = manager.initialize_env(rou, csv)\n",
    "print(env_kwargs)"
   ],
   "id": "3268ffa1ca0a9774",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ppo_agent.config = best.config\n",
    "ppo_agent.build_config()"
   ],
   "id": "4c7c2003ddb31e62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_2 = ppo_agent.train()",
   "id": "588eabfc632d79d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d05fb71f5ec4e8c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "best_result = result.get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "checkpoint_path = best_result.checkpoint.path\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "eval_env = algo.env_creator({})\n",
    "\n",
    "\n",
    "# Set up evaluation parameters\n",
    "num_episodes = 4\n",
    "\n",
    "# Evaluation loop\n",
    "episode_rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obse, _ = eval_env.reset()\n",
    "\n",
    "    \n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obse)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mean_reward = np.mean(episode_rewards)\n",
    "std_reward = np.std(episode_rewards)\n",
    "\n",
    "print(f\"Evaluation over {num_episodes} episodes:\")\n",
    "print(f\"Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "\n",
    "# Plot the results \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Clean up\n",
    "eval_env.close()\n"
   ],
   "id": "2e6014ab0a32bf0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
