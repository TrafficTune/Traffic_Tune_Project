{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2476f7866dc519ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup and Installations"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# If you are using GPU machine, run this cell\n",
    "import os\n",
    "import sys\n",
    "# Replace with your SUMO installation path\n",
    "# Set the SUMO_HOME environment variable\n",
    "os.environ['SUMO_HOME'] = '/usr/share/sumo'  # Update this path\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "os.environ['RLLIB_NUM_GPUS'] = '1'  # Set this to the number of GPUs you want to use"
   ],
   "id": "e61369b19f4fc198"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import utils\n",
    "import env_manager as env_manager\n",
    "import algo_trainer as algo_trainer\n",
    "from typing import SupportsIndex"
   ],
   "id": "325ce834970de795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Function",
   "id": "c789403d1095c702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chain_training(manager: env_manager, generator: env_manager.EnvManager.env_generator, algo_agent, running_result_grid: list):\n",
    "    if len(running_result_grid) != 0: \n",
    "        # take the best config from the previous training \n",
    "        best = running_result_grid[-1].get_best_result(metric=\"env_runners/episode_reward_mean\", mode=\"max\")\n",
    "        \n",
    "        # Initialize the environment manager with new route file\n",
    "        rou, csv = next(generator)\n",
    "        manager.initialize_env(rou, csv)\n",
    "        \n",
    "        # continue the training with the best config\n",
    "        algo_agent.config = algo_agent.from_dict(best.config)\n",
    "        algo_agent.build_config(flag=True)\n",
    "        \n",
    "    result = algo_agent.train()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def training(num_intersection: int, experiment_type: str, algo_config: str, env_config: str, num_training: SupportsIndex):\n",
    "    final_running_result = []\n",
    "    sumo_type = \"SingleAgent\"\n",
    "    algo_type = experiment_type.split(\"_\")\n",
    "     \n",
    "    if experiment_type.__contains__(\"Multi\"):\n",
    "        sumo_type = \"MultiAgent\"\n",
    "    \n",
    "    # Initialize the environment manager\n",
    "    manager = env_manager.EnvManager(f\"{sumo_type}Environment\", env_config, intersection_id=f\"intersection_{num_intersection}\")\n",
    "    generator = manager.env_generator(f\"Nets/intersection_{num_intersection}/route_xml_path_intersection_{num_intersection}.txt\", algo_name=algo_type[0])\n",
    "    \n",
    "    # Initialize the environment manager with new route file\n",
    "    rou, csv = next(generator)\n",
    "    manager.initialize_env(rou, csv)\n",
    "    \n",
    "    algo_agent = algo_trainer.ALGOTrainer(config_path=algo_config, env_manager=manager, experiment_type=experiment_type)\n",
    "    algo_agent.build_config()\n",
    "    for i in range(num_training):\n",
    "        chain_result = chain_training(manager=manager, generator=generator, algo_agent=algo_agent, running_result_grid=final_running_result)\n",
    "        if chain_result is not None:\n",
    "            final_running_result.append(chain_result)\n",
    "    \n",
    "    return final_running_result"
   ],
   "id": "57e8fffd58eb2107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Configuration",
   "id": "8b2eaa21681ad8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose which intersection you want to train\n",
    "num_intersection_to_train = 7\n",
    "\n",
    "# Choose the experiment_type:\n",
    "# PPO_SingleAgent | APPO_SingleAgent | DQN_SingleAgent | DDQN_SingleAgent | PPO_MultiAgent | APPO_MultiAgent | DQN_MultiAgent | DDQN_MultiAgent | \n",
    "experiment_type = \"APPO_MultiAgent\"  \n",
    "\n",
    "# Choose how many training cycles you want to run\n",
    "num_training_cycles = 1\n",
    "\n",
    "env_config_file_path = \"Config/env_config.json\"\n",
    "\n",
    "ppo_config_file_path = \"Config/ppo_config.json\"\n",
    "\n",
    "dqn_config_file_path = \"Config/dqn_config.json\"\n",
    "\n",
    "appo_config_file_path = \"Config/appo_config.json\""
   ],
   "id": "37bc9c4d24c7df87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Execution",
   "id": "2c691bc0107dbfe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = training(num_intersection=num_intersection_to_train, experiment_type=experiment_type, algo_config=appo_config_file_path, env_config=env_config_file_path, num_training=num_training_cycles)\n",
    "\n",
    "print(f\"Finished training for intersection: {num_intersection_to_train} with {num_training_cycles} training rounds\")"
   ],
   "id": "f3a7770d49b9be8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Notify when done\n",
    "experiment_date = env_manager.datetime.now().strftime(\"%m.%d-%H:%M:%S\")\n",
    "message = f'Training for intersection {num_intersection_to_train} with {experiment_type} and {num_training_cycles} from {experiment_date} is done!'  # Replace with your message\n",
    "recipient = 'eviatar109@icloud.com'  # Replace with your iCloud email\n",
    "utils.send_imessage(message, recipient)"
   ],
   "id": "94ed8a9c699744d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "6b814760ea5ed964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = results[-1]\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "best_result = result.get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "checkpoint_path = best_result.checkpoint.path\n",
    "print(f'Best checkpoint path: {checkpoint_path}')\n",
    "\n",
    "# Load the Algorithm from the checkpoint\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "\n",
    "# Retrieve the current configuration\n",
    "new_config = algo.config.copy()\n",
    "new_config[\"evaluation_duration\"] = 2 # Define as many evaluation episodes as you want\n",
    "\n",
    "\n",
    "# Re-create the algorithm instance with the updated configuration\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "algo.config = new_config\n",
    "\n",
    "# Evaluate the Algorithm\n",
    "eval_results = algo.evaluate()\n",
    "print(eval_results)"
   ],
   "id": "b969ba9404d829af",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
