{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a145c81c2dbf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Traffic Tune - Optimizing Traffic Signals with Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the Traffic Tune POC notebook. Our project focused on optimizing traffic signal control using reinforcement learning. Traffic congestion is a major problem in urban areas, leading to increased travel times, fuel consumption, and pollution. Traditional traffic signal control systems often struggle to adapt to dynamic traffic conditions, resulting in suboptimal traffic flow.\n",
    "\n",
    "Traffic Tune is a recommendation system that leverages reinforcement learning to dynamically adjust traffic signals at intersections. By learning from traffic patterns in real-time, Traffic Tune aims to improve traffic flow, reduce congestion, and enhance overall transportation efficiency.\n",
    "\n",
    "In this POC, we will demonstrate how to train a reinforcement learning agent to optimize traffic signal control in a simulated environment. We will use the SUMO (Simulation of Urban MObility) traffic simulation tool and the Stable Baselines3 library to train a Deep Q-Network (DQN) agent to learn an optimal traffic signal control policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476f7866dc519ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup and Installations"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import traci\n",
    "\n",
    "import env_manager as env_manager\n",
    "import algo_trainer as algo_trainer\n",
    "from typing import SupportsIndex"
   ],
   "id": "325ce834970de795",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chain_training(manager: env_manager, generator: env_manager.EnvManager.env_generator, algo_agent, running_result: list):\n",
    "    if len(running_result) != 0: \n",
    "        # take the best config from the previous training \n",
    "        best = running_result[-1].get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "        \n",
    "        # Initialize the environment manager with new route file\n",
    "        rou, csv = next(generator)\n",
    "        manager.initialize_env(rou, csv)\n",
    "        \n",
    "        # continue the training with the best config\n",
    "        algo_agent.config = best.config\n",
    "        algo_agent.build_config()\n",
    "    \n",
    "    result = algo_agent.train()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def training(num_intersection: int, experiment_type: str, algo_config: str, env_config: str, num_training: SupportsIndex):\n",
    "    running_result = []\n",
    "    sumo_type = \"SingleAgent\"\n",
    "    algo_type = experiment_type.split(\"_\")\n",
    "     \n",
    "    if experiment_type.__contains__(\"Multi\"):\n",
    "        sumo_type = \"MultiAgent\"\n",
    "    \n",
    "    # Initialize the environment manager\n",
    "    manager = env_manager.EnvManager(f\"{sumo_type}Environment\", env_config, intersection_id=f\"intersection_{num_intersection}\")\n",
    "    generator = manager.env_generator(f\"Nets/intersection_{num_intersection}/route_xml_path_intersection_{num_intersection}.txt\", algo_name=algo_type[0])\n",
    "    \n",
    "    # Initialize the environment manager with new route file\n",
    "    rou, csv = next(generator)\n",
    "    manager.initialize_env(rou, csv)\n",
    "    \n",
    "    algo_agent = algo_trainer.ALGOTrainer(config_path=algo_config, env_manager=manager, experiment_type=experiment_type)\n",
    "    algo_agent.build_config()\n",
    "\n",
    "    for i in range(num_training):\n",
    "        chain_result = chain_training(manager=manager, generator=generator, algo_agent=algo_agent, running_result=running_result)\n",
    "        if chain_result is not None:\n",
    "            running_result.append(chain_result)\n",
    "    \n",
    "    return running_result"
   ],
   "id": "57e8fffd58eb2107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_intersection_to_train = 5  # Choose which intersection you want to train\n",
    "\n",
    "# Choose the experiment_type:\n",
    "# PPO_SingleAgent | DQN_SingleAgent | DDQN_SingleAgent | PPO_MultiAgent | DQN_MultiAgent | DDQN_MultiAgent\n",
    "experiment_type = \"DQN_SingleAgent\"  \n",
    "\n",
    "num_training_cycles = 1\n",
    "\n",
    "env_config_file_path = \"env_config.json\"\n",
    "\n",
    "ppo_config_file_path = \"ppo_config.json\"\n",
    "\n",
    "dqn_config_file_path = \"dqn_config.json\""
   ],
   "id": "c1aea9551bb990a7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results = training(num_intersection=num_intersection_to_train, experiment_type=experiment_type, algo_config=dqn_config_file_path, env_config=env_config_file_path, num_training=num_training_cycles)",
   "id": "63800ac021cb3fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "python script to send imessage when the training is done\n",
    "you can write in var message the message you want to send\n",
    "and in var recipient the recipient email or phone number with country code which connected to the icloud account\n",
    "for now, it's manual but we can use it as a function which will be called whenever we want\n",
    "\"\"\"\n",
    "\n",
    "# import subprocess\n",
    "# \n",
    "# def send_imessage(message, recipient):\n",
    "#     apple_script = f'''\n",
    "#     tell application \"Messages\"\n",
    "#         set targetService to 1st service whose service type = iMessage\n",
    "#         set targetBuddy to buddy \"{recipient}\" of targetService\n",
    "#         send \"{message}\" to targetBuddy\n",
    "#     end tell\n",
    "#     '''\n",
    "#     subprocess.run(['osascript', '-e', apple_script])\n",
    "# \n",
    "# \n",
    "# # Notify when done\n",
    "# message = 'if you got this message, the training is done! send whatsapp to matan'\n",
    "# recipient = 'eviatar109@icloud.com'  # Replace with your iCloud email\n",
    "# send_imessage(message, recipient)"
   ],
   "id": "94ed8a9c699744d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = results[0]\n",
    "result1 = result[0]\n",
    "custom_metrics = result1.metrics\n",
    "print(custom_metrics)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df = pd.DataFrame(custom_metrics)\n",
    "df.to_csv(f\"Outputs/Training/intersection_{num_intersection_to_train}/experiments/{experiment_type}_intersection_{num_intersection_to_train}.csv\")\n",
    "\n",
    " \n",
    "# result1 = results_1[0]\n",
    "# result1 = result1[0]\n",
    "# print(\"DDQN\\n\",result1.metrics,\"\\n\\n\")\n",
    "# \n",
    "# result2 = results_2[0]\n",
    "# result2 = result2[0]\n",
    "# print(\"PPO\\n\",result2.metrics)\n"
   ],
   "id": "4941fe2a1ffc0e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "best_result = result.get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "checkpoint_path = best_result.checkpoint.path\n",
    "algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "eval_env = algo.env_creator({})\n",
    "\n",
    "\n",
    "# Set up evaluation parameters\n",
    "num_episodes = 4\n",
    "\n",
    "# Evaluation loop\n",
    "episode_rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obse, _ = eval_env.reset()\n",
    "\n",
    "    \n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obse)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mean_reward = np.mean(episode_rewards)\n",
    "std_reward = np.std(episode_rewards)\n",
    "\n",
    "print(f\"Evaluation over {num_episodes} episodes:\")\n",
    "print(f\"Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "\n",
    "# Plot the results \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Clean up\n",
    "eval_env.close()\n"
   ],
   "id": "87b542f3d7006d8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(eval_env)",
   "id": "f8d72db3661f7743",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fd881297538b3200",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
