{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a145c81c2dbf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Traffic Tune - Optimizing Traffic Signals with Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the Traffic Tune POC notebook. Our project focused on optimizing traffic signal control using reinforcement learning. Traffic congestion is a major problem in urban areas, leading to increased travel times, fuel consumption, and pollution. Traditional traffic signal control systems often struggle to adapt to dynamic traffic conditions, resulting in suboptimal traffic flow.\n",
    "\n",
    "Traffic Tune is a recommendation system that leverages reinforcement learning to dynamically adjust traffic signals at intersections. By learning from traffic patterns in real-time, Traffic Tune aims to improve traffic flow, reduce congestion, and enhance overall transportation efficiency.\n",
    "\n",
    "In this POC, we will demonstrate how to train a reinforcement learning agent to optimize traffic signal control in a simulated environment. We will use the SUMO (Simulation of Urban MObility) traffic simulation tool and the Stable Baselines3 library to train a Deep Q-Network (DQN) agent to learn an optimal traffic signal control policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476f7866dc519ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup and Installations"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:44:29.830311Z",
     "start_time": "2024-07-22T15:44:27.815848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import env_manager as env_manager\n",
    "import ppo_trainer as ppo_trainer"
   ],
   "id": "325ce834970de795",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:44:29.835214Z",
     "start_time": "2024-07-22T15:44:29.831278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def first_link_train(num_intersection: int, experiment_type: str, ppo_config: str, env_config: str, num_routes_files: int):\n",
    "    running_result = []\n",
    "    # Initialize the environment manager\n",
    "    manager = env_manager.EnvManager(f\"{experiment_type}Environment\", env_config,\n",
    "                                     json_id=f\"intersection_{num_intersection}\")\n",
    "    generator = manager.env_generator(\n",
    "        f\"Nets/intersection_{num_intersection}/route_xml_path_intersection_{num_intersection}.txt\")\n",
    "    rou, csv = next(generator)\n",
    "    env_kwargs = manager.initialize_env(rou, csv)\n",
    "\n",
    "    print(f\"\\nEnv creat for intersection_{num_intersection}\",\n",
    "          \"\\nNet path:\", manager.kwargs[\"net_file\"],\n",
    "          \"\\nRoute path:\", rou,\n",
    "          \"\\nCsv path:\", csv)\n",
    "\n",
    "    # Initialize the PPO agent\n",
    "    ppo_agent = ppo_trainer.PPOTrainer(ppo_config, manager, experiment_type=experiment_type)\n",
    "    ppo_agent.build_config()\n",
    "\n",
    "    # Train the PPO agent\n",
    "    result = ppo_agent.train()\n",
    "    running_result.append(result)\n",
    "    \n",
    "    if num_routes_files != 0:\n",
    "        for i in range(num_routes_files-1):\n",
    "            chain_result = chain_training(manager, generator, ppo_agent, running_result)\n",
    "            if chain_result is not None:\n",
    "                running_result.append(chain_result)\n",
    "    \n",
    "    return running_result\n",
    "\n",
    "def chain_training(manager: env_manager, generator: env_manager.EnvManager.env_generator, ppo_agent: ppo_trainer,\n",
    "                   running_result: list):\n",
    "    # take the best config from the previous training \n",
    "    best = running_result[-1].get_best_result(\"env_runners/episode_reward_max\", \"max\")\n",
    "\n",
    "    # Initialize the environment manager with new route file\n",
    "    rou, csv = next(generator)\n",
    "    env_kwargs = manager.initialize_env(rou, csv)\n",
    "    \n",
    "    print(f\"\\nEnv creat\",\n",
    "          \"\\nNet path:\", manager.kwargs[\"net_file\"],\n",
    "          \"\\nRoute path:\", rou,\n",
    "          \"\\nCsv path:\", csv)\n",
    "    \n",
    "    # continue the training with the best config\n",
    "    ppo_agent.config = best.config\n",
    "    ppo_agent.build_config()\n",
    "    \n",
    "    result = ppo_agent.train()\n",
    "    return result"
   ],
   "id": "57e8fffd58eb2107",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:44:29.837575Z",
     "start_time": "2024-07-22T15:44:29.835969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_intersection_to_train = 6  # Choose which intersection you want to train\n",
    "\n",
    "experiment_type = \"SingleAgent\"  # Choose the experiment_type: SingleAgent | MultiAgent\n",
    "\n",
    "num_routes_to_train = 2\n",
    "\n",
    "env_config_file_path = \"env_config.json\"\n",
    "\n",
    "ppo_config_file_path = \"ppo_config.json\""
   ],
   "id": "c1aea9551bb990a7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results = first_link_train(num_intersection=num_intersection_to_train, experiment_type=experiment_type, ppo_config=ppo_config_file_path, env_config=env_config_file_path, num_routes_files=num_routes_to_train)",
   "id": "63800ac021cb3fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:46:51.270983Z",
     "start_time": "2024-07-22T15:46:51.267258Z"
    }
   },
   "cell_type": "code",
   "source": "print(results)",
   "id": "4941fe2a1ffc0e09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResultGrid<[\n",
      "  Result(\n",
      "    metrics={'evaluation': {'env_runners': {'episode_reward_max': -590.59, 'episode_reward_min': -640.47, 'episode_reward_mean': -613.62, 'episode_len_mean': 720.0, 'episode_media': {}, 'episodes_timesteps_total': 2160, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-590.59, -609.8, -640.47], 'episode_lengths': [720, 720, 720]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.6092773918915897, 'mean_inference_ms': 0.40721189213678693, 'mean_action_processing_ms': 0.04515549800295456, 'mean_env_wait_ms': 27.777304401711945, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0037590662638346353, 'StateBufferConnector_ms': 0.0024477640787760415, 'ViewRequirementAgentConnector_ms': 0.05824565887451172}, 'num_episodes': 3, 'episode_return_max': -590.59, 'episode_return_min': -640.47, 'episode_return_mean': -613.62, 'episodes_this_iter': 3}, 'num_agent_steps_sampled_this_iter': 2160, 'num_env_steps_sampled_this_iter': 2160, 'timesteps_this_iter': 2160, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 1, 'num_remote_worker_restarts': 0}, 'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.1666861374269832, 'cur_kl_coeff': 0.04999999999999999, 'cur_lr': 2.0000000000000005e-05, 'total_loss': 1.7842103454199705, 'policy_loss': 0.015025283531709151, 'vf_loss': 7.507090741937811, 'vf_explained_var': 0.004362495379014448, 'kl': 0.0010508195335728705, 'entropy': 1.0764016118916597, 'entropy_coeff': 0.09999999999999998}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 64.0, 'num_grad_updates_lifetime': 275.5, 'diff_num_grad_updates_vs_sampler_policy': 54.5}}, 'num_env_steps_sampled': 2160, 'num_env_steps_trained': 2160, 'num_agent_steps_sampled': 2160, 'num_agent_steps_trained': 2160, 'num_env_steps_sampled_for_evaluation_this_iter': 2160}, 'env_runners': {'episode_reward_max': -590.89, 'episode_reward_min': -603.0, 'episode_reward_mean': -597.7866666666666, 'episode_len_mean': 720.0, 'episode_media': {}, 'episodes_timesteps_total': 2160, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-603.0, -590.89, -599.47], 'episode_lengths': [720, 720, 720]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.6150809796081278, 'mean_inference_ms': 0.39710607041871543, 'mean_action_processing_ms': 0.0446801590891236, 'mean_env_wait_ms': 27.552838606983396, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.003226598103841146, 'StateBufferConnector_ms': 0.002209345499674479, 'ViewRequirementAgentConnector_ms': 0.05691051483154297}, 'num_episodes': 1, 'episode_return_max': -590.89, 'episode_return_min': -603.0, 'episode_return_mean': -597.7866666666666, 'episodes_this_iter': 1}, 'num_healthy_workers': 1, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2160, 'num_agent_steps_trained': 2160, 'num_env_steps_sampled': 2160, 'num_env_steps_trained': 2160, 'num_env_steps_sampled_this_iter': 720, 'num_env_steps_trained_this_iter': 720, 'num_env_steps_sampled_throughput_per_sec': 27.533226240692365, 'num_env_steps_trained_throughput_per_sec': 27.533226240692365, 'num_env_steps_sampled_lifetime': 2160, 'num_agent_steps_sampled_lifetime': 2160, 'num_steps_trained_this_iter': 720, 'agent_timesteps_total': 2160, 'timers': {'training_iteration_time_ms': 22358.695, 'restore_workers_time_ms': 0.009, 'training_step_time_ms': 22358.669, 'sample_time_ms': 22152.917, 'load_time_ms': 0.362, 'load_throughput': 1987211.371, 'learn_time_ms': 203.298, 'learn_throughput': 3541.593, 'synch_weights_time_ms': 1.907, 'restore_eval_workers_time_ms': 0.006, 'evaluation_iteration_time_ms': 63122.161, 'evaluation_iteration_throughput': 34.219}, 'counters': {'num_env_steps_sampled': 2160, 'num_env_steps_trained': 2160, 'num_agent_steps_sampled': 2160, 'num_agent_steps_trained': 2160, 'num_env_steps_sampled_for_evaluation_this_iter': 2160}, 'perf': {'cpu_util_percent': 34.97631578947369, 'ram_util_percent': 69.26052631578948}},\n",
      "    path='/Users/eviat/Desktop/Final_Project/Traffic_Tune_Project/Outputs/Training/intersection_6/saved_agent/PPO_2024-07-22_18-44-31/PPO_PPO_49c5c_00000_0_2024-07-22_18-44-31',\n",
      "    filesystem='local',\n",
      "    checkpoint=Checkpoint(filesystem=local, path=/Users/eviat/Desktop/Final_Project/Traffic_Tune_Project/Outputs/Training/intersection_6/saved_agent/PPO_2024-07-22_18-44-31/PPO_PPO_49c5c_00000_0_2024-07-22_18-44-31/checkpoint_000002)\n",
      "  )\n",
      "]>]\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
